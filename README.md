CS-MSASR
🎯 CS-MSASR: Multimodal Sentiment Analysis and Speech Recognition Dataset for Video Based on Changsha Dialects
<p align="center"> <img src="https://github.com/user-attachments/assets/537b1f36-9fe7-4ab8-8a13-2efbdba669c2" alt="CS-MSASR Dataset Preview" width="600"/> </p>

📌 Motivation
Intelligent human-computer interaction systems should not only focus on mainstream languages but also understand regional dialects that are rich in emotional and cultural characteristics.

Changsha is the capital of Hunan province in China. The Changsha dialect, as one of the major dialects in southern China, features variable intonation and vivid expressions, but is almost absent in current AI corpora.

📂 Dataset Overview
We introduce CS-MSASR, the first multimodal video dataset for Changsha dialects, targeting sentiment analysis and speech recognition.

🎥 1085 video clips across diverse real-life scenarios

🗣️ Speakers aged 8 to 93, ensuring diversity

🧾 Each clip is manually transcribed with authentic Changsha dialect text

❤️ 5-level multimodal sentiment labels:
negative, weakly negative, neutral, weakly positive, positive

🧠 Unimodal sentiment annotations for:

Text

Audio

Visual

✂️ Fine-grained temporal segmentation

📊 Benchmark
We evaluated:

12 mainstream multimodal sentiment analysis models

5 speech recognition models using:

Direct inference

Fine-tuning on CS-MSASR

📎 Citation (Coming Soon)

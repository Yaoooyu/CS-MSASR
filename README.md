# CS-MSASR
CS-MSASR: Multimodal Sentiment Analysis and Speech Recognition Dataset for Video Based on Changsha Dialects
![a823f9a39f70aba81d3c82ba3b60059](https://github.com/user-attachments/assets/537b1f36-9fe7-4ab8-8a13-2efbdba669c2)
Intelligent human-computer interaction systems should not only focus on mainstream languages but also understand regional dialects that are rich in emotional and cultural characteristics. Changsha is the capital of Hunan province in China, and Changsha dialect, as one of the important dialects in the southern region of China, has variable intonation and vivid expressions, but it is almost missing in the current Artificial Intelligence (AI) corpus. To this end, we release the first multimodal video dataset CS-MSASR for Changsha dialect for sentiment analysis and speech recognition research. The dataset contains 1085 video clips covering a rich variety of real-life scenarios with a high diversity of speakers spanning from 8 to 93 years old. Each video is human-annotated with authentic textual content and contains five categories of multimodal sentiment labels (negative, weakly negative, neutral, weakly positive, and positive), as well as each unimodal sentiment label (text, audio, and visual) under fine-grained segmentation. This experiment evaluated 12 mainstream multimodal sentiment analysis baseline models on CS-MSASR, along with direct inference and fine-tuning using 5 speech recognition models. 

CS-MSASR
ğŸ¯ CS-MSASR: Multimodal Sentiment Analysis and Speech Recognition Dataset for Video Based on Changsha Dialects
<p align="center"> <img src="https://github.com/user-attachments/assets/537b1f36-9fe7-4ab8-8a13-2efbdba669c2" alt="CS-MSASR Dataset Preview" width="600"/> </p>

ğŸ“Œ Motivation
Intelligent human-computer interaction systems should not only focus on mainstream languages but also understand regional dialects that are rich in emotional and cultural characteristics.

Changsha is the capital of Hunan province in China. The Changsha dialect, as one of the major dialects in southern China, features variable intonation and vivid expressions, but is almost absent in current AI corpora.

ğŸ“‚ Dataset Overview
We introduce CS-MSASR, the first multimodal video dataset for Changsha dialects, targeting sentiment analysis and speech recognition.

ğŸ¥ 1085 video clips across diverse real-life scenarios

ğŸ—£ï¸ Speakers aged 8 to 93, ensuring diversity

ğŸ§¾ Each clip is manually transcribed with authentic Changsha dialect text

â¤ï¸ 5-level multimodal sentiment labels:
negative, weakly negative, neutral, weakly positive, positive

ğŸ§  Unimodal sentiment annotations for:

Text

Audio

Visual

âœ‚ï¸ Fine-grained temporal segmentation

ğŸ“Š Benchmark
We evaluated:

12 mainstream multimodal sentiment analysis models

5 speech recognition models using:

Direct inference

Fine-tuning on CS-MSASR

ğŸ“ Citation (Coming Soon)

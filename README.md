## 🎯 CS-MSASR: 
# Multimodal Sentiment Analysis and Speech Recognition Dataset for Video Based on Changsha Dialects

<p align="center">
  <img src="https://github.com/user-attachments/assets/537b1f36-9fe7-4ab8-8a13-2efbdba669c2" alt="CS-MSASR Dataset Preview" width="600"/>
</p>

---

### 📌 Background

Intelligent human-computer interaction systems should not only focus on mainstream languages but also understand **regional dialects** that are rich in emotional and cultural characteristics. Changsha is the capital of Hunan province in China, and the **Changsha dialect**, as one of the significant dialects in southern China, features **variable intonation** and **vivid expressions**, but is **almost absent** in current Artificial Intelligence (AI) corpora.

---

### 📂 Dataset Overview

We introduce **CS-MSASR**, the **first multimodal video dataset** for the Changsha dialect, aimed at **sentiment analysis** and **speech recognition** research.

- 🎥 **1085 video clips** covering diverse real-life scenarios
- 🗣️ Speakers ranging from **8 to 93 years old**, ensuring diversity
- 🧾 Each video is **manually transcribed** with authentic Changsha dialect text
- ❤️ **5 categories of multimodal sentiment labels**:
  - `Negative`
  - `Weakly Negative`
  - `Neutral`
  - `Weakly Positive`
  - `Positive`
- 🧠 **Unimodal sentiment annotations** for:
  - Text
  - Audio
  - Visual
- ✂️ **Fine-grained temporal segmentation**

---

### 📊 Benchmark

We evaluated:

- **12 mainstream multimodal sentiment analysis models**
- **5 speech recognition models** using:
  - Direct inference
  - Fine-tuning on CS-MSASR

---

### 📎 Citation

If you use the CS-MSASR dataset in your research, please cite the following paper:

